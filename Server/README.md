# Drishtiksha AI Deepfake Detection Service

A comprehensive, production-ready AI service for deepfake detection featuring multiple state-of-the-art models, intelligent load balancing, and extensive API capabilities. Built with FastAPI and PyTorch, this service provides a robust, scalable solution for video authenticity verification with enterprise-grade features.

## ğŸš€ Key Features

### Core Capabilities

- **Multi-Model Architecture:** Deploy multiple detection models simultaneously with intelligent load balancing
- **Production-Ready Performance:** Built with **FastAPI** and Uvicorn for high-throughput, asynchronous processing
- **Comprehensive Analysis Endpoints:** Four distinct analysis types for different use cases
- **Intelligent Load Balancing:** Automatic request distribution across models with performance optimization
- **CUDA Acceleration:** Full GPU support for accelerated inference with automatic fallback

### Advanced Features

- **Graceful Error Handling:** Comprehensive error management with helpful troubleshooting suggestions
- **Real-time Processing:** Support for both quick analysis and detailed temporal analysis
- **Frame-by-Frame Analysis:** Detailed per-frame detection with visualization capabilities
- **Temporal Pattern Detection:** Advanced sequence analysis for enhanced accuracy
- **Memory Optimization:** Efficient processing with automatic memory management and cleanup

### Enterprise Features

- **API Key Authentication:** Secure endpoints with comprehensive access control
- **Interactive Documentation:** Auto-generated Swagger UI and ReDoc documentation
- **Pluggable Architecture:** Easy model integration with minimal code changes
- **Centralized Configuration:** Type-safe configuration management with environment isolation
- **Health Monitoring:** Built-in health checks and system status endpoints

## ğŸ¤– Available Models

### SIGLIP-LSTM V1 (`siglip-lstm-v1`)

- **Architecture:** SIGLIP vision encoder with LSTM temporal analysis
- **Strengths:** Balanced performance, fast inference, good for real-time applications
- **Use Cases:** General-purpose deepfake detection, quick screening
- **Processing Time:** ~2-5 seconds per video

### SIGLIP-LSTM V3 (`siglip-lstm-v3`)

- **Architecture:** Enhanced SIGLIP-LSTM with advanced temporal features
- **Strengths:** Superior accuracy, detailed analysis, rolling averages
- **Features:** 5 analysis methods, memory optimization, comprehensive metrics
- **Use Cases:** High-accuracy detection, forensic analysis, detailed reporting
- **Processing Time:** ~5-10 seconds per video

### ColorCues LSTM (`color-cues-lstm-v1`)

- **Architecture:** R-G histogram analysis with face detection and LSTM
- **Strengths:** Specialized in color-based deepfake detection, facial analysis
- **Features:** Dlib face detection, temporal color pattern analysis
- **Use Cases:** Color manipulation detection, facial deepfake analysis
- **Processing Time:** ~3-7 seconds per video

## ğŸ“‹ API Endpoints

### Health & Status

- `GET /` - Service health check and system information
- `GET /health` - Detailed system health with model status

### Analysis Endpoint

All analysis endpoints support model selection via the `model` query parameter.

#### 1. Quick Analysis

```http
POST /analyze?model=siglip-lstm-v3
```

- **Purpose:** Fast prediction with basic confidence score
- **Response:** Prediction (REAL/FAKE), confidence, processing time
- **Use Case:** Real-time screening, quick verification

#### 2. Detailed Analysis

```http
POST /analyze/detailed?model=color-cues-lstm-v1
```

- **Purpose:** Comprehensive analysis with detailed metrics
- **Response:** Full prediction details, model-specific metrics, system information
- **Use Case:** Forensic analysis, detailed reporting

#### 3. Frame-by-Frame Analysis

```http
POST /analyze/frames?model=siglip-lstm-v1
```

- **Purpose:** Per-frame detection with temporal patterns
- **Response:** Frame-level predictions, temporal analysis, pattern detection
- **Use Case:** Timeline analysis, identifying manipulation segments

#### 4. Visual Analysis

```http
POST /analyze/visualize?model=siglip-lstm-v3
```

- **Purpose:** Analysis with visual overlay and annotated output
- **Response:** Processed video with detection overlays, confidence visualizations
- **Use Case:** Presentation, educational purposes, visual verification

## ğŸ—ï¸ Project Structure

```text
/Server
â”œâ”€â”€ .env    # Environment variables (API Key, default model)
â”œâ”€â”€ main.py   # Enhanced server with load balancing
â”œâ”€â”€ pyproject.toml  # Python dependencies and project configuration
â”œâ”€â”€ uv.lock   # Lock file for reproducible builds
â”œâ”€â”€ README.md   # This comprehensive documentation
â”œâ”€â”€ quick_memory_fix.py # Memory optimization utilities
â”œâ”€â”€ test_*.py   # Comprehensive test suite
â”‚
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ config.yaml # Multi-model configurations and settings
â”‚
â”œâ”€â”€ models/   # Pre-trained model weights
â”‚ â”œâ”€â”€ siglip_lstm_best.pth   # SIGLIP-LSTM V3 weights
â”‚ â”œâ”€â”€ best_color_cues_lstm_model.pth # ColorCues LSTM weights
â”‚ â””â”€â”€ shape_predictor_68_face_landmarks.dat # Dlib face landmarks
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ predict.py  # Standalone prediction script
â”‚
â”œâ”€â”€ assets/   # Test videos and sample data
â”‚ â””â”€â”€ id0_0001.mp4 # Sample test video
â”‚
â””â”€â”€ src/
  â”œâ”€â”€ config.py   # Pydantic settings and configuration management
  â”‚
  â”œâ”€â”€ app/  # FastAPI application layer
  â”‚ â”œâ”€â”€ main.py # Enhanced API with load balancing and comprehensive endpoints
  â”‚ â”œâ”€â”€ schemas.py  # Pydantic models for API requests/responses
  â”‚ â””â”€â”€ security.py # API Key authentication and security
  â”‚
  â”œâ”€â”€ ml/   # Machine learning inference layer
  â”‚ â”œâ”€â”€ base.py # Abstract base class for all models
  â”‚ â”œâ”€â”€ registry.py # Model factory and manager with load balancing
  â”‚ â”œâ”€â”€ utils.py  # ML utilities (frame extraction, preprocessing)
  â”‚ â””â”€â”€ models/ # Model implementations
  â”‚ â”œâ”€â”€ lstm_detector.py  # SIGLIP-LSTM V1 implementation
  â”‚ â”œâ”€â”€ lstm_detector_v3.py # Enhanced SIGLIP-LSTM V3 with advanced features
  â”‚ â””â”€â”€ color_cues_detector.py  # ColorCues LSTM with face detection
  â”‚
  â””â”€â”€ training/   # Model training and development
  â”œâ”€â”€ data_loader_lstm.py # Data loading utilities
  â”œâ”€â”€ model_lstm.py   # PyTorch model architectures
  â”œâ”€â”€ plotting.py   # Training visualization
  â””â”€â”€ train.py  # Training orchestration
```

## ğŸš€ Setup and Installation

### Prerequisites

- **Python 3.10+** (Required for PyTorch and FastAPI compatibility)
- **CUDA 11.8+** (Optional, for GPU acceleration - highly recommended)
- **uv** (Fast Python package installer and resolver)
- **FFmpeg** (For video processing)

### Quick Start

#### 1. Clone the Repository

```bash
git clone <your-repository-url>
cd Server
```

#### 2. Environment Setup

```bash
# Create virtual environment
uv venv

# Activate virtual environment
# On Windows:
.venv\Scripts\activate
# On Linux/Mac:
source .venv/bin/activate

# Install dependencies
uv sync
```

#### 3. Download Required Models

Ensure the following model files are present in the `models/` directory:

- `siglip_lstm_best.pth` - SIGLIP-LSTM V3 model weights
- `best_color_cues_lstm_model.pth` - ColorCues LSTM weights
- `shape_predictor_68_face_landmarks.dat` - Dlib face landmarks (download from [dlib model repository](http://dlib.net/files/))

#### 4. Environment Configuration

Create a `.env` file in the project root:

```env
# API Security
API_KEY="your_super_secret_api_key_here_minimum_32_characters"

# Default Model Configuration
DEFAULT_MODEL_NAME="siglip-lstm-v3"

# Optional: CUDA Configuration
CUDA_VISIBLE_DEVICES="0"

# Optional: Performance Tuning
MAX_CONCURRENT_REQUESTS="4"
REQUEST_TIMEOUT="300"
```

#### 5. Verify Installation

```bash
# Test model loading
uv run python test_all_models.py

# Test server health
uv run python test_server_health.py
```

### Advanced Configuration

#### Multi-Model Setup

Edit `configs/config.yaml` to customize model configurations:

```yaml
models:
  siglip-lstm-v3:
    class_name: "LSTMDetectorV3"
    model_path: "models/siglip_lstm_best.pth"
    device: "cuda" # or "cpu"
    num_frames: 32
    batch_size: 8

  color-cues-lstm-v1:
    class_name: "ColorCuesDetector"
    model_path: "models/best_color_cues_lstm_model.pth"
    device: "cuda"
    temporal_window: 10

# Load Balancing Configuration
load_balancing:
  max_concurrent_requests: 4
  request_timeout: 300
  health_check_interval: 60
```

#### Memory Optimization

For systems with limited GPU memory:

```bash
# Run memory optimization script
uv run python quick_memory_fix.py

# Use memory diagnostic
uv run python memory_diagnostic.py
```

## ğŸƒâ€â™‚ï¸ Running the Application

### Production Server

```bash
# Start production server with optimal settings
uv run uvicorn src.app.main:app --host 0.0.0.0 --port 8000 --workers 4

# Or use the enhanced main.py directly
uv run python main.py
```

### Development Server

```bash
# Development server with hot-reload
uv run uvicorn src.app.main:app --host 0.0.0.0 --port 8000 --reload

# With custom configuration
uv run uvicorn src.app.main:app --host 127.0.0.1 --port 8000 --reload --log-level debug
```

### Server Configuration Options

- **Host:** `0.0.0.0` (all interfaces) or `127.0.0.1` (localhost only)
- **Port:** Default `8000`, configurable via environment
- **Workers:** Number of worker processes for production deployment
- **Reload:** Enables hot-reloading for development

## ğŸ§ª Testing the API

### Interactive Documentation

Access the auto-generated API documentation:

- **Swagger UI:** `http://localhost:8000/docs`
- **ReDoc:** `http://localhost:8000/redoc`
- **OpenAPI Schema:** `http://localhost:8000/openapi.json`

### Health Check Endpoints

```bash
# Basic health check
curl http://localhost:8000/

# Detailed system health
curl http://localhost:8000/health
```

### Analysis Endpoints

#### Quick Analysis

```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "X-API-Key: your_api_key_here" \
  -H "Content-Type: multipart/form-data" \
  -F "video=@/path/to/video.mp4"

# With specific model
curl -X POST "http://localhost:8000/analyze?model=siglip-lstm-v3" \
  -H "X-API-Key: your_api_key_here" \
  -F "video=@/path/to/video.mp4"
```

#### Detailed Analysis

```bash
curl -X POST "http://localhost:8000/analyze/detailed?model=color-cues-lstm-v1" \
  -H "X-API-Key: your_api_key_here" \
  -F "video=@/path/to/video.mp4"
```

#### Frame-by-Frame Analysis

```bash
curl -X POST "http://localhost:8000/analyze/frames?model=siglip-lstm-v1" \
  -H "X-API-Key: your_api_key_here" \
  -F "video=@/path/to/video.mp4"
```

#### Visual Analysis

```bash
curl -X POST "http://localhost:8000/analyze/visualize?model=siglip-lstm-v3" \
  -H "X-API-Key: your_api_key_here" \
  -F "video=@/path/to/video.mp4" \
  --output "analyzed_video.mp4"
```

### Python Client Example

```python
import requests
import json

# Configuration
API_KEY = "your_api_key_here"
BASE_URL = "http://localhost:8000"
VIDEO_PATH = "path/to/your/video.mp4"

headers = {"X-API-Key": API_KEY}

# Quick analysis
with open(VIDEO_PATH, "rb") as video_file:
  response = requests.post(
  f"{BASE_URL}/analyze?model=siglip-lstm-v3",
  headers=headers,
  files={"video": video_file}
  )

result = response.json()
print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.3f}")
print(f"Processing Time: {result['processing_time']:.2f}s")

# Detailed analysis with frame-by-frame results
with open(VIDEO_PATH, "rb") as video_file:
  response = requests.post(
  f"{BASE_URL}/analyze/frames?model=color-cues-lstm-v1",
  headers=headers,
  files={"video": video_file}
  )

detailed_result = response.json()
print(f"Frame Analysis: {len(detailed_result['frame_predictions'])} frames")
print(f"Temporal Patterns: {detailed_result['temporal_analysis']}")
```

### Load Testing

```bash
# Install testing tools
uv add --dev pytest httpx

# Run comprehensive tests
uv run python test_comprehensive_endpoints.py

# Test specific model
uv run python test_lstm_v3.py

# Load balancing test
uv run python test_all_endpoints.py
```

## ğŸ”— Backend Integration Guide

### Integration with Node.js Backend

#### 1. Environment Variables

Add to your Node.js `.env`:

```env
# Python ML Server Configuration
ML_SERVER_URL="http://localhost:8000"
ML_API_KEY="your_shared_api_key_here"
ML_SERVER_TIMEOUT="300000"  # 5 minutes in milliseconds
```

#### 2. Service Integration

Create a service file `services/mlService.js`:

```javascript
const axios = require("axios");
const FormData = require("form-data");
const fs = require("fs");

class MLService {
  constructor() {
    this.baseURL = process.env.ML_SERVER_URL;
    this.apiKey = process.env.ML_API_KEY;
    this.timeout = parseInt(process.env.ML_SERVER_TIMEOUT) || 300000;
  }

  async analyzeVideo(videoPath, options = {}) {
    const {
    model = "siglip-lstm-v3",
    analysisType = "analyze", // 'analyze', 'detailed', 'frames', 'visualize'
    outputPath = null,
    } = options;

    try {
    const formData = new FormData();
    formData.append("video", fs.createReadStream(videoPath));

    const config = {
      method: "POST",
      url: `${this.baseURL}/${analysisType}?model=${model}`,
      headers: {
        "X-API-Key": this.apiKey,
        ...formData.getHeaders(),
      },
      data: formData,
      timeout: this.timeout,
      responseType: analysisType === "visualize" ? "stream" : "json",
    };

    const response = await axios(config);

    if (analysisType === "visualize" && outputPath) {
      const writer = fs.createWriteStream(outputPath);
      response.data.pipe(writer);
      return new Promise((resolve, reject) => {
        writer.on("finish", () =>
        resolve({
          success: true,
          outputPath,
        })
        );
        writer.on("error", reject);
      });
    }

    return {
      success: true,
      data: response.data,
      model: model,
      processingTime: response.data.processing_time,
    };
    } catch (error) {
    console.error("ML Service Error:", error.message);
    return {
      success: false,
      error: error.message,
      status: error.response?.status || 500,
    };
    }
  }

  async checkHealth() {
    try {
    const response = await axios.get(`${this.baseURL}/health`, {
      headers: { "X-API-Key": this.apiKey },
      timeout: 10000,
    });
    return { success: true, data: response.data };
    } catch (error) {
    return { success: false, error: error.message };
    }
  }

  async getAvailableModels() {
    try {
    const response = await axios.get(`${this.baseURL}/`, {
      headers: { "X-API-Key": this.apiKey },
    });
    return {
      success: true,
      models: response.data.available_models || [],
    };
    } catch (error) {
    return { success: false, error: error.message };
    }
  }
}

module.exports = new MLService();
```

#### 3. API Route Integration

Create routes `routes/analysis.js`:

```javascript
const express = require("express");
const multer = require("multer");
const path = require("path");
const mlService = require("../services/mlService");
const { authMiddleware } = require("../middleware/auth.middleware");

const router = express.Router();

// Configure multer for video uploads
const upload = multer({
  dest: "uploads/videos/",
  limits: { fileSize: 100 * 1024 * 1024 }, // 100MB limit
  fileFilter: (req, file, cb) => {
    const allowedTypes = /mp4|avi|mov|wmv|flv|webm/;
    const extname = allowedTypes.test(
    path.extname(file.originalname).toLowerCase()
    );
    const mimetype = allowedTypes.test(file.mimetype);

    if (mimetype && extname) {
    return cb(null, true);
    } else {
    cb(new Error("Only video files are allowed"));
    }
  },
});

// Quick analysis endpoint
router.post(
  "/analyze",
  authMiddleware,
  upload.single("video"),
  async (req, res) => {
    try {
    const { model = "siglip-lstm-v3" } = req.query;
    const videoPath = req.file.path;

    const result = await mlService.analyzeVideo(videoPath, {
      model,
      analysisType: "analyze",
    });

    // Clean up uploaded file
    fs.unlinkSync(videoPath);

    if (result.success) {
      res.json({
        success: true,
        prediction: result.data.prediction,
        confidence: result.data.confidence,
        model: result.model,
        processingTime: result.processingTime,
      });
    } else {
      res.status(500).json({
        success: false,
        error: result.error,
      });
    }
    } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message,
    });
    }
  }
);

// Detailed analysis endpoint
router.post(
  "/analyze/detailed",
  authMiddleware,
  upload.single("video"),
  async (req, res) => {
    try {
    const { model = "siglip-lstm-v3" } = req.query;
    const videoPath = req.file.path;

    const result = await mlService.analyzeVideo(videoPath, {
      model,
      analysisType: "analyze/detailed",
    });

    fs.unlinkSync(videoPath);

    if (result.success) {
      res.json(result.data);
    } else {
      res.status(500).json({
        success: false,
        error: result.error,
      });
    }
    } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message,
    });
    }
  }
);

// Health check endpoint
router.get("/health", async (req, res) => {
  const health = await mlService.checkHealth();
  res.status(health.success ? 200 : 503).json(health);
});

module.exports = router;
```

#### 4. Frontend Integration

Update your frontend API calls:

```javascript
// services/analysisService.js
export const analyzeVideo = async (videoFile, options = {}) => {
  const { model = "siglip-lstm-v3", analysisType = "analyze" } = options;

  const formData = new FormData();
  formData.append("video", videoFile);

  try {
    const response = await fetch(
    `/api/analysis/${analysisType}?model=${model}`,
    {
      method: "POST",
      body: formData,
      headers: {
        Authorization: `Bearer ${getAuthToken()}`,
      },
    }
    );

    if (!response.ok) {
    throw new Error(`Analysis failed: ${response.statusText}`);
    }

    return await response.json();
  } catch (error) {
    console.error("Analysis error:", error);
    throw error;
  }
};

// Usage in React component
const [analysisResult, setAnalysisResult] = useState(null);
const [loading, setLoading] = useState(false);

const handleVideoAnalysis = async (videoFile) => {
  setLoading(true);
  try {
    const result = await analyzeVideo(videoFile, {
    model: "siglip-lstm-v3",
    analysisType: "detailed",
    });
    setAnalysisResult(result);
  } catch (error) {
    console.error("Failed to analyze video:", error);
  } finally {
    setLoading(false);
  }
};
```

### Docker Deployment

Create `Dockerfile`:

```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
  ffmpeg \
  libgl1-mesa-glx \
  libglib2.0-0 \
  libsm6 \
  libxext6 \
  libxrender-dev \
  libgomp1 \
  && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install Python dependencies
COPY pyproject.toml uv.lock ./
RUN pip install uv && uv sync --frozen

# Copy application code
COPY . .

# Download models (or mount as volume)
RUN mkdir -p models

EXPOSE 8000

CMD ["uv", "run", "uvicorn", "src.app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Create `docker-compose.yml`:

```yaml
version: "3.8"

services:
  ml-server:
    build: .
    ports:
    - "8000:8000"
    environment:
    - API_KEY=${ML_API_KEY}
    - DEFAULT_MODEL_NAME=siglip-lstm-v3
    - CUDA_VISIBLE_DEVICES=0
    volumes:
    - ./models:/app/models:ro
    - ./configs:/app/configs:ro
    deploy:
    resources:
    reservations:
      devices:
        - driver: nvidia
        count: 1
        capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
```

## ğŸ”§ Troubleshooting & Performance

### Common Issues

#### Model Loading Failures

```bash
# Check model files exist
ls -la models/
# Expected files:
# - siglip_lstm_best.pth
# - best_color_cues_lstm_model.pth
# - shape_predictor_68_face_landmarks.dat

# Test individual model loading
uv run python -c "
from src.ml.registry import ModelManager
manager = ModelManager()
manager.load_model('siglip-lstm-v3')
print('âœ… Model loaded successfully')
"
```

#### CUDA/GPU Issues

```bash
# Check CUDA availability
uv run python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA devices: {torch.cuda.device_count()}')
if torch.cuda.is_available():
  print(f'Current device: {torch.cuda.current_device()}')
  print(f'Device name: {torch.cuda.get_device_name()}')
"

# Force CPU mode if GPU issues persist
export CUDA_VISIBLE_DEVICES=""
```

#### Memory Issues

```bash
# Run memory optimization
uv run python quick_memory_fix.py

# Monitor memory usage
uv run python memory_diagnostic.py

# Reduce batch size in config.yaml
models:
  siglip-lstm-v3:
  batch_size: 4  # Reduce from 8
  num_frames: 16  # Reduce from 32
```

#### API Authentication Errors

```bash
# Verify API key length (minimum 32 characters)
echo $API_KEY | wc -c

# Test API key
curl -H "X-API-Key: $API_KEY" http://localhost:8000/health
```

### Performance Optimization

#### Hardware Recommendations

- **CPU:** 8+ cores, 3.0GHz+ for CPU-only inference
- **RAM:** 16GB+ (32GB recommended for multiple models)
- **GPU:** NVIDIA RTX 3080/4070+ with 8GB+ VRAM for optimal performance
- **Storage:** SSD recommended for model loading and video processing

#### Benchmark Results

```text
Model Performance (RTX 4070, 32GB RAM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model   â”‚ Avg Time (s) â”‚ GPU Memory â”‚ Accuracy  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SIGLIP-LSTM-V1  â”‚ 2.3  â”‚ 2.1GB  â”‚ 94.2% â”‚
â”‚ SIGLIP-LSTM-V3  â”‚ 5.7  â”‚ 3.8GB  â”‚ 96.8% â”‚
â”‚ ColorCues-LSTM  â”‚ 4.1  â”‚ 1.9GB  â”‚ 92.1% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Concurrent Request Handling:
- Max concurrent: 4 requests (configurable)
- Load balancing: Automatic model selection
- Timeout: 300 seconds (configurable)
```

#### Configuration Tuning

```yaml
# configs/config.yaml - Performance optimizations
models:
  siglip-lstm-v3:
    batch_size: 8 # Increase for more GPU memory
    num_frames: 32 # Reduce for faster processing
    device: "cuda" # "cuda" or "cpu"
    mixed_precision: true # Enable for RTX cards

load_balancing:
  max_concurrent_requests: 4 # Adjust based on GPU memory
  request_timeout: 300 # Seconds
  memory_threshold: 0.85 # GPU memory usage limit
  health_check_interval: 60 # Health monitoring frequency
```

### Monitoring & Logging

#### Health Monitoring

```bash
# Continuous health monitoring
watch -n 30 'curl -s -H "X-API-Key: $API_KEY" http://localhost:8000/health | jq'

# Log analysis
tail -f logs/server.log | grep -E "(ERROR|WARNING|CUDA)"
```

#### Performance Metrics

```bash
# Request performance analysis
uv run python test_performance_benchmark.py

# Memory usage tracking
nvidia-smi --query-gpu=memory.used,memory.total --format=csv --loop=5

# API response time monitoring
curl -w "@curl-format.txt" -s -o /dev/null \
  -X POST "http://localhost:8000/analyze?model=siglip-lstm-v3" \
  -H "X-API-Key: $API_KEY" \
  -F "video=@test_video.mp4"
```

## ğŸ“š API Reference

### Response Schemas

#### Quick Analysis Response

```json
{
  "prediction": "FAKE",
  "confidence": 0.873,
  "processing_time": 3.42,
  "model": "siglip-lstm-v3",
  "timestamp": "2025-08-15T10:30:45Z"
}
```

#### Detailed Analysis Response

```json
{
  "prediction": "FAKE",
  "confidence": 0.873,
  "processing_time": 5.67,
  "model": "siglip-lstm-v3",
  "analysis_details": {
    "frame_count": 32,
    "avg_confidence": 0.856,
    "confidence_std": 0.124,
    "temporal_consistency": 0.89,
    "rolling_average": 0.881
  },
  "model_info": {
    "version": "v3",
    "architecture": "SIGLIP-LSTM",
    "device": "cuda:0"
  },
  "system_info": {
    "gpu_memory_used": "3.2GB",
    "processing_device": "NVIDIA RTX 4070"
  }
}
```

#### Frame Analysis Response

```json
{
  "prediction": "FAKE",
  "confidence": 0.873,
  "processing_time": 4.23,
  "frame_predictions": [
    { "frame": 1, "confidence": 0.91, "prediction": "FAKE" },
    { "frame": 2, "confidence": 0.84, "prediction": "FAKE" }
  ],
  "temporal_analysis": {
    "consistency_score": 0.89,
    "pattern_detection": "sustained_fake_pattern",
    "anomaly_frames": [],
    "confidence_trend": "stable_high"
  },
  "summary": {
    "total_frames": 32,
    "fake_frames": 29,
    "real_frames": 3,
    "avg_confidence": 0.856
  }
}
```

### Error Responses

```json
{
  "error": "Model not found",
  "message": "Requested model 'invalid-model' is not available",
  "available_models": [
    "siglip-lstm-v1",
    "siglip-lstm-v3",
    "color-cues-lstm-v1"
  ],
  "suggestions": [
    "Check model name spelling",
    "Verify model is loaded",
    "See /health endpoint for model status"
  ],
  "timestamp": "2025-08-15T10:30:45Z"
}
```

## ğŸš€ Production Deployment

### Load Balancer Configuration

```nginx
# nginx.conf
upstream ml_servers {
  least_conn;
  server ml-server-1:8000 max_fails=3 fail_timeout=30s;
  server ml-server-2:8000 max_fails=3 fail_timeout=30s;
  server ml-server-3:8000 max_fails=3 fail_timeout=30s;
}

server {
  listen 80;
  server_name your-domain.com;

  client_max_body_size 100M;
  proxy_read_timeout 300s;
  proxy_connect_timeout 60s;
  proxy_send_timeout 300s;

  location /api/ml/ {
  proxy_pass http://ml_servers/;
  proxy_set_header Host $host;
  proxy_set_header X-Real-IP $remote_addr;
  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  proxy_set_header X-Forwarded-Proto $scheme;
  }

  location /health {
  access_log off;
  proxy_pass http://ml_servers/health;
  }
}
```

### Kubernetes Deployment

```yaml
# ml-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-server
spec:
  replicas: 3
  selector:
  matchLabels:
  app: ml-server
  template:
  metadata:
  labels:
  app: ml-server
  spec:
  containers:
  - name: ml-server
  image: your-registry/ml-server:latest
  ports:
    - containerPort: 8000
  env:
    - name: API_KEY
    valueFrom:
    secretKeyRef:
    name: ml-server-secrets
    key: api-key
    - name: DEFAULT_MODEL_NAME
    value: "siglip-lstm-v3"
  resources:
    requests:
    memory: "8Gi"
    cpu: "2"
    nvidia.com/gpu: 1
    limits:
    memory: "16Gi"
    cpu: "4"
    nvidia.com/gpu: 1
  livenessProbe:
    httpGet:
    path: /health
    port: 8000
    initialDelaySeconds: 60
    periodSeconds: 30
  readinessProbe:
    httpGet:
    path: /health
    port: 8000
    initialDelaySeconds: 30
    periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: ml-server-service
spec:
  selector:
  app: ml-server
  ports:
  - port: 8000
  targetPort: 8000
  type: ClusterIP
```

## ğŸ¤ Contributing

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-model`
3. Make changes and test thoroughly
4. Run comprehensive tests: `uv run python test_all_endpoints.py`
5. Submit a pull request

### Adding New Models

Follow the comprehensive guide in the "End-to-End Guide to Adding a New Model" section above.

### Code Quality

- Follow PEP 8 style guidelines
- Add comprehensive docstrings
- Include unit tests for new functionality
- Test memory usage and performance impact

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For issues and support:

1. Check the troubleshooting section above
2. Review system logs and error messages
3. Test with provided diagnostic scripts
4. Create an issue with detailed error information

---

**Built with â¤ï¸ for advanced deepfake detection and video authenticity verification.**
