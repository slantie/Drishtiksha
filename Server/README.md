# Drishtiksha AI Deepfake Detection Service

A high-performance, modular, and extensible API service for deepfake detection, built with FastAPI and PyTorch. This project is designed with a pluggable architecture, allowing developers to easily add, train, and deploy new detection models with minimal changes to the core API code.

## Key Features

-   **High-Performance API:** Built with **FastAPI** and Uvicorn for asynchronous, high-throughput performance.
-   **Pluggable Model Architecture:** Easily add new detection models by creating a new inference class and updating the configuration. The API is completely decoupled from the model logic.
-   **Secure Endpoint:** The analysis endpoint is protected by API Key authentication.
-   **Centralized & Typed Configuration:** All settings are managed in a single `config.yaml` and a `.env` file, loaded into a type-safe Pydantic model.
-   **Isolated Training Environment:** All code related to model training (data loaders, training loops, plotting) is neatly organized and separated from the production inference code.
-   **Automatic API Documentation:** Interactive API documentation (Swagger UI & ReDoc) is automatically generated by FastAPI.

## Project Structure

```
/Server
|
|-- .env                    # Environment variables (API Key, default model)
|-- requirements.txt        # Python dependencies
|-- README.md               # This file
|
|-- configs/
|   └── config.yaml         # All project configurations (models, training, etc.)
|
|-- models/                 # Pre-trained model weights (e.g., .pth files)
|
|-- scripts/
|   └── predict.py          # Standalone script for testing predictions
|
|-- src/
|   ├── config.py           # Pydantic settings management
|   |
|   ├── app/                # FastAPI application layer
|   |   ├── main.py         # API entrypoint, routes, lifespan
|   |   ├── schemas.py      # Pydantic data models for API I/O
|   |   └── security.py     # API Key dependency logic
|   |
|   ├── ml/                 # Core machine learning inference layer
|   |   ├── base.py         # Abstract Base Class for all models
|   |   ├── registry.py     # Model factory and manager
|   |   ├── utils.py        # ML-specific utility functions (e.g., frame extraction)
|   |   └── models/         # Implementations of specific detectors
|   |       └── lstm_detector.py
|   |
|   └── training/           # Isolated package for training models
|       ├── data_loader_lstm.py
|       ├── model_lstm.py   # PyTorch nn.Module definitions
|       ├── plotting.py
|       └── train.py        # The main training script
```

## Setup and Installation

### 1. Prerequisites

-   Python 3.10+
-   `uv` (a fast Python package installer and resolver)

### 2. Clone the Repository

```bash
git clone <your-repository-url>
cd Server
```

### 3. Set Up Environment and Install Dependencies

We use `uv` to create a virtual environment and install all required packages.

```bash
# Create a virtual environment named .venv
uv venv

# Activate the virtual environment
source .venv/bin/activate

# Install all dependencies from requirements.txt
uv sync
```

### 4. Configure the Environment

You must create a `.env` file in the project root to store your secrets and set the default model.

1.  Create a file named `.env`: `touch .env`
2.  Add the following content, replacing the placeholder values:

    ```env
    # Service API Key for authenticating requests
    API_KEY="your_super_secret_and_long_api_key_here"

    # The name of the default model to load on startup (must match a key in configs/config.yaml)
    DEFAULT_MODEL_NAME="siglip-lstm-v1"
    ```

## Running the Application

### Running the API Server

With your virtual environment activated, run the following command from the project root:

```bash
uv run uvicorn src.app.main:app --host 0.0.0.0 --port 5000 --reload
```

-   The server will be running at `http://0.0.0.0:5000`.
-   The `--reload` flag enables hot-reloading for development.

### Testing the API

You can access the auto-generated interactive documentation by navigating to `http://localhost:5000/docs` in your browser.

To test the protected `/analyze` endpoint, you can use `curl` or a tool like Postman.

**`curl` Example:**

```bash
curl -X POST "http://localhost:5000/analyze" \
-H "X-API-Key: your_super_secret_and_long_api_key_here" \
-F "video=@/path/to/your/test_video.mp4"
```

---

## End-to-End Guide to Adding a New Model

This project is designed to be easily extensible. Follow these steps to define, train, and deploy a new model.

Let's assume we want to create a new detector named `ResNetGRUDetector`.

### Step 1: Create the Model Definition

This is the core `torch.nn.Module` that defines your model's architecture.

1.  Create a new file: `src/training/model_resnet_gru.py`.
2.  Define your PyTorch model inside. This is where your deep learning architecture lives.

    ```python
    # src/training/model_resnet_gru.py
    import torch.nn as nn
    from torchvision.models import resnet18

    class ResNetGRUClassifier(nn.Module):
        def __init__(self, config):
            super().__init__()
            self.config = config
            # Example architecture
            self.backbone = resnet18(weights='IMAGENET1K_V1')
            # Replace the final layer
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()

            self.gru = nn.GRU(
                input_size=num_features,
                hidden_size=config['gru_hidden_size'],
                num_layers=config['gru_num_layers'],
                batch_first=True,
                bidirectional=True
            )
            self.classifier = nn.Linear(config['gru_hidden_size'] * 2, 1)

        def forward(self, pixel_values, num_frames_per_video=1):
            # ... your model's forward pass logic ...
            # features = self.backbone(pixel_values) ... etc.
            # logits = self.classifier(...)
            # return logits

    def create_resnet_gru_model(config):
        return ResNetGRUClassifier(config)
    ```

### Step 2: Create the Inference Class

This class acts as a wrapper around your model for use in the API. It inherits from `BaseModel` and handles loading weights and running predictions.

1.  Create a new file: `src/ml/models/resnet_gru_detector.py`.
2.  Implement the `load` and `predict` methods.

    ```python
    # src/ml/models/resnet_gru_detector.py
    import torch
    from src.ml.base import BaseModel
    from src.ml.utils import extract_frames
    # Import the model creation factory from your training code
    from src.training.model_resnet_gru import create_resnet_gru_model
    import time

    class ResNetGRUDetector(BaseModel):
        def load(self):
            """Loads the ResNet+GRU model and its weights."""
            self.device = self.config.get("device", "cpu")
            model_def_config = self.config.get("model_definition", {})

            # Use the factory function to build the model structure
            self.model = create_resnet_gru_model(model_def_config)
            # Load your trained weights
            self.model.load_state_dict(torch.load(self.config["model_path"], map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            print(f"✅ ResNetGRUDetector loaded successfully.")

        def predict(self, video_path: str):
            """Runs inference using the ResNet+GRU model."""
            start_time = time.time()
            frames = extract_frames(video_path, self.config["num_frames"])
            if not frames:
                raise ValueError("Could not extract frames.")
            
            # --- Your prediction logic here ---
            # 1. Preprocess frames (if needed, e.g., with torchvision transforms)
            # 2. Convert to tensors and move to device
            # 3. Run model.forward()
            # 4. Apply sigmoid and calculate confidence
            # ---
            
            processing_time = time.time() - start_time
            prediction = "FAKE" # Placeholder
            confidence = 0.99   # Placeholder

            return {
                "prediction": prediction,
                "confidence": confidence,
                "processing_time": processing_time,
            }
    ```

### Step 3: Register the New Model

Make the model manager aware of your new `ResNetGRUDetector`.

1.  Open `src/ml/registry.py`.
2.  Import your new detector and add it to the `MODEL_REGISTRY` dictionary.

    ```python
    # src/ml/registry.py
    from src.ml.models.lstm_detector import LSTMDetector
    from src.ml.models.resnet_gru_detector import ResNetGRUDetector # <-- 1. IMPORT

    MODEL_REGISTRY: Dict[str, Type[BaseModel]] = {
        "LSTMDetector": LSTMDetector,
        "ResNetGRUDetector": ResNetGRUDetector, # <-- 2. ADD TO DICTIONARY
    }
    ```

### Step 4: Add Model Configuration

Define the parameters for your new model in the master config file.

1.  Open `configs/config.yaml`.
2.  Add a new entry under the `models` key for `resnet-gru-v1`.

    ```yaml
    # configs/config.yaml
    models:
      siglip-lstm-v1:
        # ... (existing config)
      
      # Add your new model's configuration
      resnet-gru-v1:
        class_name: "ResNetGRUDetector" # Must match the key in MODEL_REGISTRY
        description: "A detector using ResNet18 and a bidirectional GRU."
        model_path: "models/resnet_gru_deepfake_detector_best.pth"
        processor_path: null # This model might not need a Hugging Face processor
        num_frames: 64
        model_definition:
          # Parameters needed by the ResNetGRUClassifier __init__ method
          gru_hidden_size: 256
          gru_num_layers: 2
    ```

### Step 5: Train the New Model

1.  **Update Training Target:** In `configs/config.yaml`, change the `target_model` under the `training` section to point to your new model.

    ```yaml
    # configs/config.yaml
    training:
      target_model: "resnet-gru-v1" # <-- Change this
      # ... (update data paths and other parameters as needed)
    ```

2.  **Run the Training Script:** Execute the main training script. It will now use your new `ResNetGRUClassifier` and `resnet-gru-v1` configuration.

    ```bash
    uv run python src/training/train.py
    ```
    This will eventually save your best model weights to `models/resnet_gru_deepfake_detector_best.pth`.

### Step 6: Deploy the New Model in the API

This is the final, easiest step.

1.  Open your `.env` file.
2.  Change `DEFAULT_MODEL_NAME` to the key of your new model from `config.yaml`.

    ```env
    # .env
    DEFAULT_MODEL_NAME="resnet-gru-v1"
    ```

3.  Restart the API server. It will now automatically load your `ResNetGRUDetector` and use it for all `/analyze` requests.