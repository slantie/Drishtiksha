# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNZiBUbP15so4i5dqI9hTOhTzWdDzquK
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# -*- coding: utf-8 -*-
import os
from imutils import face_utils
import tensorflow as tf
import dlib
import imutils
import numpy as np
import cv2
from scipy.spatial import distance as dist
from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import (Input, TimeDistributed, GlobalAveragePooling2D,
                                     LSTM, Dropout, Dense)
from tensorflow.keras.models import Model

# --- Constants and Configuration ---
# ‼️ IMPORTANT: Update these paths before running ‼️
SHAPE_PREDICTOR_PATH = "/content/drive/MyDrive/dlib_models/shape_predictor_68_face_landmarks.dat"
MODEL_WEIGHTS_PATH = "/content/drive/MyDrive/my_models/cnn+LSTM_EAR-2_resume.keras" # Path to your saved .keras or .h5 weights file

# Model and Preprocessing settings
IMG_SIZE = (160, 160)
SEQ_LEN = 10from scipy.spatial import distance as dist

    # Define the sequential input
    seq_input = Input(shape=(sequence_length, *img_size, 3))

    # Wrap the CNN base in a TimeDistributed layer to apply it to each frame of the sequence
    x = TimeDistributed(cnn_base)(seq_input)

    # Flatten the features for each frame before feeding to the LSTM
    x = TimeDistributed(GlobalAveragePooling2D())(x)

    # LSTM layer to process the sequence of features
    x = LSTM(128)(x)

    # Dropout for regularization
    x = Dropout(0.3)(x)

    # Final classification layer
    out = Dense(1, activation="sigmoid")(x)

    # Compile the model
    model = Model(seq_input, out)
    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    print("Model architecture built successfully.")
    return model

# --- Helper Functions ---
def calculate_EAR(eye):
    """
    Calculates the Eye Aspect Ratio (EAR) for a given set of eye landmarks.
    """
    y1 = dist.euclidean(eye[1], eye[5])
    y2 = dist.euclidean(eye[2], eye[4])
    x1 = dist.euclidean(eye[0], eye[3])
    if x1 == 0:
        return 0.0
    return (y1 + y2) / x1

def preprocess_frame(frame):
    """
    Resizes and normalizes a single frame for model prediction.
    """
    if frame is None:
        return None
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    resized_frame = cv2.resize(frame_rgb, IMG_SIZE)
    return resized_frame.astype(np.float32) / 255.0

# --- Core Processing Function ---mail
def process_and_predict_video(video_path, shape_predictor, model):
    """
    Processes a video to detect blinks, creates sequences of blink frames,
    and predicts whether the video is real or fake using the loaded model.
    """
    print(f"Starting processing for video: {os.path.basename(video_path)}...")

    detector = dlib.get_frontal_face_detector()
    (L_start, L_end) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (R_start, R_end) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]

    cam = cv2.VideoCapture(video_path)
    if not cam.isOpened():
        print("Error: Could not open video file.")
        return

    count_frame = 0
    all_blink_frames = []
    blink_frame_buffer = []

    while True:
        ret, frame = cam.read()
        if not ret:
            break

        frame = imutils.resize(frame, width=640)
        (frame_h, frame_w) = frame.shape[:2]
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = detector(gray)

        if len(faces) > 0:
            face = faces[0]  # Assume one face
            shape = shape_predictor(gray, face)
            shape = face_utils.shape_to_np(shape)

            lefteye = shape[L_start:L_end]
            righteye = shape[R_start:R_end]

            all_eye_landmarks = np.concatenate((lefteye, righteye))
            (x, y, w, h) = cv2.boundingRect(all_eye_landmarks)
            padding = 15
            eye_crop = frame[max(0, y - padding):min(frame_h, y + h + padding),
                             max(0, x - padding):min(frame_w, x + w + padding)]

            avg_EAR = (calculate_EAR(lefteye) + calculate_EAR(righteye)) / 2.0

            if avg_EAR < BLINK_THRESH:
                count_frame += 1
                if eye_crop.size > 0:
                    blink_frame_buffer.append(eye_crop)
            else:
                if count_frame >= SUCC_FRAME:
                    all_blink_frames.extend(blink_frame_buffer)
                count_frame = 0
                blink_frame_buffer = []

    cam.release()
    print(f"Blink detection complete. Found {len(all_blink_frames)} frames across all blinks.")

    if len(all_blink_frames) < SEQ_LEN:
        print(f"Warning: Only {len(all_blink_frames)} blink frames were detected. "
              f"Need at least {SEQ_LEN} to form a sequence. Cannot predict.")
        return

    processed_frames = [preprocess_frame(f) for f in all_blink_frames if f is not None]

    sequences = []
    for i in range(len(processed_frames) - SEQ_LEN + 1):
        seq = processed_frames[i:i + SEQ_LEN]
        sequences.append(seq)

    if not sequences:
        print("Warning: Could not build any sequences from the detected frames. Prediction aborted.")
        return

    sequences_np = np.array(sequences)
    print(f"Successfully built {len(sequences_np)} sequences for prediction.")

    predictions = model.predict(sequences_np, verbose=0).ravel()
    avg_confidence = float(np.mean(predictions))
    final_class = "REAL" if avg_confidence >= 0.5 else "FAKE"

    print("\n" + "="*34)
    print("   Final Video Classification   ")
    print("="*34)
    print(f"🎬 Predicted Class: {final_class}")
    print(f"📈 Confidence Score: {avg_confidence:.4f}")
    print("(Closer to 1.0 is REAL, closer to 0.0 is FAKE)")
    print("="*34 + "\n")

# --- Main Execution Block ---
if __name__ == "__main__":
    #parser = argparse.ArgumentParser(description="Classify a video as REAL or FAKE based on eye blinks.")
    #parser.add_argument("--video", type=str, required=True, help="Path to the input video file.")
    #args = parser.parse_args()
    vid_path = "/content/drive/MyDrive/test/Celeb_Fake.mp4"
    # Validate file paths
    if not os.path.exists(vid_path):
        print(f"Error: Video file not found at '{vid_path}'")
    elif not os.path.exists(SHAPE_PREDICTOR_PATH):
        print(f"Error: dlib shape predictor not found at '{SHAPE_PREDICTOR_PATH}'")
    elif not os.path.exists(MODEL_WEIGHTS_PATH):
        print(f"Error: Model weights file not found at '{MODEL_WEIGHTS_PATH}'")
    else:
        try:
            # 1. Build the model architecture
            model = create_model()

            # 2. Load the pre-trained weights into the architecture
            print(f"Loading trained weights from: {MODEL_WEIGHTS_PATH}")
            model.load_weights(MODEL_WEIGHTS_PATH)
            print("Weights loaded successfully.")

            # 3. Load the dlib shape predictor
            shape_predictor = dlib.shape_predictor(SHAPE_PREDICTOR_PATH)

            # 4. Run the main processing and prediction pipeline
            process_and_predict_video(vid_path, shape_predictor, model)

        except Exception as e:
            print(f"An unexpected error occurred: {e}")