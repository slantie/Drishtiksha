# configs/config.yaml

# ===================================================================
# Drishtiksha AI - Main Application Configuration (REFACTORED)
# ===================================================================
# This file defines the core settings for the inference server.
# It is loaded and validated by `src/config.py` at startup.
# -------------------------------------------------------------------

project_name: "Drishtiksha: Deepfake Detection"

# --- MODEL CONFIGURATIONS ---
# Each key is a unique model identifier used in API calls and the .env file.
# The `class_name` MUST now match the exact Python class name for auto-discovery.

models:
  SIGLIP-LSTM-V1:
    # CHANGE: Must match the Python class name `SiglipLSTMV1`
    class_name: "SiglipLSTMV1"
    description: "SigLIP feature extractor with a bidirectional LSTM head for temporal analysis."
    model_path: "models/SigLip-LSTM-v1.pth"
    processor_path: "google/siglip-base-patch16-224"
    num_frames: 120
    isAudio: false
    isImage: false
    isVideo: true
    model_definition:
      base_model_path: "google/siglip-base-patch16-224"
      lstm_hidden_size: 512
      lstm_num_layers: 2
      num_classes: 1

  SIGLIP-LSTM-V3:
    # CHANGE: Must match the Python class name `SiglipLSTMV3`
    class_name: "SiglipLSTMV3"
    description: "Enhanced SigLIP-LSTM detector with advanced visualization and metrics collection."
    model_path: "models/SigLip-LSTM-v3.pth"
    processor_path: "google/siglip-base-patch16-224"
    num_frames: 120
    rolling_window_size: 10
    isAudio: false
    isImage: false
    isVideo: true
    model_definition:
      base_model_path: "google/siglip-base-patch16-224"
      lstm_hidden_size: 512
      lstm_num_layers: 2
      num_classes: 1

  SIGLIP-LSTM-V4:
    # CHANGE: Must match the Python class name `SiglipLSTMV4`
    class_name: "SiglipLSTMV4"
    description: "V4 SigLIP-LSTM with deeper classifier heads and dropout for improved regularization."
    model_path: "models/SigLip-LSTM-v4.pth"
    processor_path: "google/siglip-base-patch16-224"
    num_frames: 120
    rolling_window_size: 10
    isAudio: false
    isVideo: true
    isImage: false
    model_definition:
      base_model_path: "google/siglip-base-patch16-224"
      lstm_hidden_size: 512
      lstm_num_layers: 2
      num_classes: 1
      dropout_rate: 0.5

  COLOR-CUES-LSTM-V1:
    # CHANGE: Must match the Python class name `ColorCuesLSTMV1`
    class_name: "ColorCuesLSTMV1"
    description: "ColorCues LSTM detector using R-G color histogram analysis for deepfake detection."
    model_path: "models/ColorCues-LSTM-v1.pth"
    dlib_model_path: "models/Face-Landmarks.dat"
    isAudio: false
    isVideo: true
    isImage: false
    sequence_length: 32
    frames_per_video: 50
    histogram_bins: 32
    landmark_margin: 20
    rolling_window_size: 10
    hidden_size: 64
    dropout: 0.5

  EFFICIENTNET-B7-V1:
    # CHANGE: Must match the Python class name `EfficientNetB7Detector`
    class_name: "EfficientNetB7Detector"
    description: "Frame-by-frame face detector using MTCNN and EFFICIENTNET-B7 classifier."
    model_path: "models/EfficientNet-B7-v1"
    encoder: "tf_efficientnet_b7.ns_jft_in1k"
    input_size: 380
    isAudio: false
    isImage: false
    isVideo: true

  EYEBLINK-CNN-LSTM-V1:
    # CHANGE: Must match the Python class name `EyeblinkDetectorV1`
    class_name: "EyeblinkDetectorV1"
    description: "Detects deepfakes by analyzing sequences of eye blinks using a CNN LSTM architecture."
    model_path: "models/EyeBlink-CNN-LSTM-v1.pth"
    dlib_model_path: "models/Face-Landmarks.dat"
    sequence_length: 10
    blink_threshold: 0.45
    consecutive_frames: 2
    # REMOVED: `isDetailed` is no longer needed.
    isAudio: false
    isImage: false
    isVideo: true
    model_definition:
      base_model_name: "legacy_xception"
      lstm_hidden_size: 128
      dropout_rate: 0.3
      img_size: [160, 160]

  SCATTERING-WAVE-V1:
    # CHANGE: Must match the Python class name `ScatteringWaveV1`
    class_name: "ScatteringWaveV1"
    description: "Audio deepfake detector using a Wavelet Scattering Transform on Mel Spectrograms."
    model_path: "models/Scattering-Wave-v1.pth"
    # REMOVED: `isDetailed` is no longer needed.
    isAudio: true
    isImage: false
    isVideo: false
    sampling_rate: 16000
    duration_seconds: 2.0
    image_size: [256, 256]

  MEL-SPECTROGRAM-CNN-V1:
    class_name: "MelSpectrogramCNNV1"
    description: "Audio deepfake detector using a Wavelet Scattering Transform on Mel Spectrograms."
    model_path: "models/MEL-Spectrogram-CNN-v1.pth"
    isAudio: true
    isImage: false
    isVideo: false
    # --- Method-specific parameters ---
    sampling_rate: 16000
    n_fft: 2048
    hop_length: 512
    n_mels: 256
    dpi: 100
    chunk_duration_s: 1.0

  STFT-SPECTROGRAM-CNN-V1:
    class_name: "STFTSpectrogramCNNV1"
    description: "Audio deepfake detector using stacked wideband and narrowband STFT spectrograms."
    model_path: "models/STFT-Spectrogram-CNN-v1.pth"
    isAudio: true
    isImage: false
    isVideo: false
    # --- Method-specific parameters ---
    sampling_rate: 16000
    n_fft_wide: 512
    n_fft_narrow: 2048
    hop_length: 160
    img_height: 512
    img_width: 256
    dpi: 100
    chunk_duration_s: 1.0

  DISTIL-DIRE-V1:
    class_name: "DistilDIREDetectorV1"
    description: "Image deepfake detector using Diffusion Reconstruction Error (DIRE) and a ResNet backbone."
    # The main detector model checkpoint
    model_path: "models/DistilDIRE-v1.pth"
    # The required ADM diffusion model for noise generation
    adm_model_path: "models/ADM-256x256-DistilDIRE.pt"
    isImage: true
    isAudio: false
    isVideo: false
    image_size: 256
    # Configuration for the ADM model, as seen in the original repository
    adm_config:
      attention_resolutions: "32,16,8"
      class_cond: False
      diffusion_steps: 1000
      image_size: 256
      learn_sigma: True
      noise_schedule: "linear"
      num_channels: 256
      num_head_channels: 64
      num_res_blocks: 2
      resblock_updown: True
      use_fp16: True
      use_scale_shift_norm: True
      timestep_respacing: "ddim20"
      clip_denoised: True
      use_ddim: True
      num_samples: 1
      real_step: 0

  MFF-MOE-V1:
    class_name: "MFFMoEDetectorV1"
    description: "Mixture-of-Experts (MoE) model combining multiple backbones for robust image and video analysis."
    model_path: "models/MFF-MoE-V1/"
    isImage: true
    isVideo: true
    isAudio: false
    video_frames_to_sample: 100
