# Drishtiksha AI - Backend Service v2.0 âœ…

## ğŸ“œ Overview

This repository contains the backend service for the Drishtiksha AI Deepfake Detection System. It is a robust **Node.js/Express** application designed to serve as the central orchestrator for user authentication, video management, and the coordination of analysis tasks with a powerful Python-based AI microservice.

This v2.0 has been completely refactored to an **asynchronous, queue-based architecture**. When a user uploads a video, the job is immediately queued for background processing, providing a responsive user experience and a highly scalable foundation. The backend then communicates with the Python ML service to perform a comprehensive, multi-model analysis, persisting all results to a **PostgreSQL** database using the **Prisma ORM**.

### ğŸ¯ **Integration Status: FULLY OPERATIONAL**

-   âœ… **Server Communication**: Health checks and API integration working perfectly
-   âœ… **Authentication**: JWT-based auth system fully functional
-   âœ… **Multi-Model AI**: All 3 models (SIGLIP-LSTM-V1, V3, Color-Cues-LSTM) integrated
-   âœ… **Analysis Pipeline**: QUICK, DETAILED, FRAMES, and VISUALIZE analyses operational
-   âœ… **Database Operations**: Comprehensive data persistence with Prisma ORM
-   âœ… **File Management**: Cloudinary integration for scalable media storage
-   âœ… **Queue System**: Asynchronous background processing with in-memory queue
-   âœ… **End-to-End Testing**: Complete workflow validation with Jest/Supertest

---

## ğŸ› ï¸ Tech Stack

### Core Technologies

-   **Node.js:** JavaScript runtime for server-side development
-   **Express:** Fast, unopinionated web framework for Node.js
-   **Prisma:** Next-generation ORM for type-safe database access
-   **PostgreSQL:** Robust relational database for data persistence
-   **JWT (JSON Web Tokens):** Secure, token-based user authentication

### Communication & Job Queuing

-   **Axios:** Promise-based HTTP client for ML server communication
-   **In-Memory Queue:** A simple, effective in-memory queue for managing background analysis jobs. (_For production scaling, this can be swapped with a Redis-based system like BullMQ._)

### Media & File Processing

-   **Multer:** Middleware for handling `multipart/form-data` (file uploads)
-   **Cloudinary:** Cloud-based service for scalable image and video management

### Development & Security

-   **Nodemon:** Development utility for auto-restarting the server
-   **Jest & Supertest:** For comprehensive unit and end-to-end API testing
-   **bcryptjs:** Secure password hashing library
-   **helmet & cors:** Essential security middleware for Express

---

## ğŸ—ï¸ Project Structure

The project follows a modern, modular structure that cleanly separates concerns.

```text
/Backend
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.js              # Express application setup and middleware
â”‚   â”œâ”€â”€ server.js           # Application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                # API route definitions, controllers, and validation
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ health/
â”‚   â”‚   â””â”€â”€ videos/
â”‚   â”‚
â”‚   â”œâ”€â”€ config/             # Configuration (e.g., database connection)
â”‚   â”œâ”€â”€ middleware/         # Express middleware (auth, errors, file uploads)
â”‚   â”œâ”€â”€ queue/              # Background job processing
â”‚   â”‚   â””â”€â”€ videoProcessorQueue.js
â”‚   â”‚
â”‚   â”œâ”€â”€ repositories/       # Data access layer (Prisma queries)
â”‚   â”‚   â”œâ”€â”€ user.repository.js
â”‚   â”‚   â””â”€â”€ video.repository.js
â”‚   â”‚
â”‚   â”œâ”€â”€ services/           # Business logic layer
â”‚   â”‚   â”œâ”€â”€ auth.service.js
â”‚   â”‚   â”œâ”€â”€ video.service.js        # Video processing coordination
â”‚   â”‚   â””â”€â”€ modelAnalysis.service.js  # ML service integration
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utility functions and helpers (errors, responses)
â”‚
â”œâ”€â”€ prisma/               # Database schema and migrations
â”‚   â””â”€â”€ schema.prisma
â”‚
â”œâ”€â”€ .env                  # Environment variables
â””â”€â”€ package.json            # Dependencies and scripts
```

---

## âœ¨ Key Features

### ğŸš€ **Production-Ready Integration**

-   **Asynchronous Analysis Workflow:** Videos are uploaded and immediately queued for background processing, providing instant feedback to the user.
-   **Dynamic Multi-Model Analysis:** The service automatically queries the Python API to find all available models and runs every supported analysis type (`QUICK`, `DETAILED`, `FRAMES`, `VISUALIZE`) for each one.
-   **Proven Reliability:** Successfully processes multiple concurrent analyses with 8+ completed analyses per video in testing.

### ğŸ”’ **Security & Authentication**

-   **Secure Authentication:** Robust user authentication and authorization using JWT with refresh tokens.
-   **API Key Protection:** Secure communication with Python ML service using API key validation.
-   **Input Validation:** Comprehensive request validation and sanitization.

### ğŸ“Š **Data Management**

-   **Scalable Media Handling:** Videos are uploaded directly to Cloudinary, keeping the application stateless and ready for scaling.
-   **Transactional Database Writes:** Analysis results, including detailed metrics and errors, are saved to the database in a single transaction, guaranteeing data consistency.
-   **Comprehensive API:** A clean, versioned REST API for user management, video CRUD, and retrieving detailed analysis results.

### ğŸ”§ **Architecture Excellence**

-   **Microservices Integration:** Seamless communication between Node.js backend and Python FastAPI ML service.
-   **Queue-Based Processing:** In-memory job queue with background processing for optimal performance.
-   **Error Handling:** Robust error handling with detailed logging and graceful failure recovery.
-   **Testing Coverage:** Complete end-to-end test suite validating the entire analysis pipeline.

---

## ğŸ—„ï¸ Database Schema

The Prisma schema is designed to capture all facets of the analysis process.

-   **`User`**: Stores user credentials and profile information.
-   **`Video`**: Tracks each uploaded video, its Cloudinary URL, and its current processing status (`QUEUED`, `PROCESSING`, `ANALYZED`, `FAILED`).
-   **`DeepfakeAnalysis`**: The central table linking a `Video` to a specific analysis result from a particular `model` and `analysisType`.
-   **Related Analysis Tables**: `AnalysisDetails`, `FrameAnalysis`, and `AnalysisError` store the rich, normalized data returned by the Python API.

---

## âš™ï¸ Installation & Setup

### Prerequisites

-   Node.js 18+
-   PostgreSQL 12+
-   A running instance of the Python Deepfake Detection Service.
-   A Cloudinary account.

### Quick Start

1.  **Navigate to Backend Directory**
    ```bash
    cd Backend
    ```
2.  **Install Dependencies**
    ```bash
    npm install
    ```
3.  **Setup Environment Variables**
    Copy the `.env.example` file to `.env` and fill in your configuration details (database URL, JWT secrets, Cloudinary credentials, and the Python server URL/API key).
    ```env
    # .env
    DATABASE_URL="postgresql://user:password@localhost:5432/drishtiksha_db"
    JWT_SECRET="your-jwt-secret"
    CLOUDINARY_CLOUD_NAME="your-cloud-name"
    CLOUDINARY_API_KEY="your-api-key"
    CLOUDINARY_API_SECRET="your-api-secret"
    SERVER_URL="http://localhost:8000"
    SERVER_API_KEY="your-python-server-api-key"
    ...
    ```
4.  **Initialize Database**
    Generate the Prisma client and push the schema to your database.
    ```bash
    npx prisma generate
    npx prisma db push
    ```
5.  **Start the Server**
    ```bash
    npm run dev
    ```

The server will start on the port defined in your `.env` file (default: 3000).

---

## ğŸ“– API Documentation

The API is versioned under `/api/v1`. All video routes require a Bearer token for authentication.

### Authentication

-   `POST /api/v1/auth/register`: Create a new user account.
-   `POST /api/v1/auth/login`: Log in to receive an `accessToken` and `refreshToken`.

### Main Workflow

The new workflow is fully asynchronous and much simpler for the client.

**Step 1: Upload a Video**
The client makes a single request to upload a video. The server immediately accepts it and queues it for background processing.

-   `POST /api/v1/videos`
    -   **Content-Type:** `multipart/form-data`
    -   **Body:**
        -   `video`: The video file.
        -   `description`: (Optional) A description for the video.
    -   **Response:** `202 Accepted` with the initial video record, which will have a status of `QUEUED`.

**Step 2: Check for Results**
The client can poll this endpoint to get the latest status and the full analysis results once they are complete. The `status` field will change from `QUEUED` -\> `PROCESSING` -\> `ANALYZED`.

-   `GET /api/v1/videos/:id`
    -   **Response:** The complete video object, including a populated `analyses` array with detailed results from all models.

### Other Endpoints

-   `GET /api/v1/videos`: Get a list of all videos for the authenticated user.
-   `DELETE /api/v1/videos/:id`: Delete a video and all its associated data.
-   `GET /api/v1/videos/status`: Check the health and status of the downstream Python ML service.

---

## âœ… Testing & Validation

The project includes a comprehensive Jest and Supertest setup for end-to-end testing that validates the complete integration pipeline.

### ğŸ¯ **Test Coverage & Results**

-   **End-to-End Integration**: Complete workflow testing from user registration to video analysis
-   **Multi-Model Validation**: All 3 AI models (SIGLIP-LSTM-V1, V3, Color-Cues-LSTM) tested
-   **Analysis Type Coverage**: QUICK, DETAILED, FRAMES, and VISUALIZE analyses verified
-   **Performance Testing**: Successfully processes 8+ concurrent analyses per video
-   **Error Handling**: Comprehensive error scenarios and recovery testing

### ğŸ† **Latest Test Results**

```
âœ… Integration test completed successfully!
ğŸ“Š Total analyses completed: 8
ğŸ” SIGLIP analyses: 5
ğŸ¨ Color cues analyses: 3
Test Suites: 1 passed, 1 total
Tests: 1 passed, 1 total
```

### ğŸš€ **Running Tests**

1. Ensure your test database is configured.
2. Make sure the Python ML service is running on `http://localhost:8000`
3. Run all tests:

    ```bash
    npm test
    ```

4. Run specific test suites:
    ```bash
    npm test tests/e2e.test.js    # End-to-end integration tests
    npm test tests/integration.test.js  # API integration tests
    npm test tests/video-endpoints.test.js  # Video API tests
    ```
